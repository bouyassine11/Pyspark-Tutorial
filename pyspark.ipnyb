{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PySpark Tutorial: Student Segmentation on Online Learning Platform\n\nThis notebook accompanies the PySpark tutorial. Run each cell sequentially to follow along.","metadata":{}},{"cell_type":"markdown","source":"## Step 1: Install PySpark (if needed)\n\nUncomment and run if you haven't installed PySpark yet:","metadata":{}},{"cell_type":"code","source":"# !pip install pyspark","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 2: Import Libraries and Create SparkSession","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.window import Window\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.evaluation import ClusteringEvaluator\n\nfrom datetime import datetime, timedelta\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set random seed for reproducibility\nrandom.seed(42)\n\nprint(\"Libraries imported successfully!\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create SparkSession\nspark = (\n    SparkSession.builder\n    .appName(\"Learning Platform Analysis\")\n    .config(\"spark.memory.offHeap.enabled\", \"true\")\n    .config(\"spark.memory.offHeap.size\", \"10g\")\n    .getOrCreate()\n)\n\nprint(f\"Spark version: {spark.version}\")\nprint(\"SparkSession created successfully!\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 3: Create Custom Dataset","metadata":{}},{"cell_type":"code","source":"# Define schema\nschema = StructType([\n    StructField(\"student_id\", IntegerType(), False),\n    StructField(\"course_id\", StringType(), False),\n    StructField(\"enrollment_date\", StringType(), False),\n    StructField(\"last_activity\", StringType(), False),\n    StructField(\"videos_watched\", IntegerType(), False),\n    StructField(\"quizzes_taken\", IntegerType(), False),\n    StructField(\"assignments_submitted\", IntegerType(), False),\n    StructField(\"forum_posts\", IntegerType(), False),\n    StructField(\"course_progress\", FloatType(), False),\n    StructField(\"course_price\", FloatType(), False)\n])\n\n# Generate sample data\ndata = []\ncourses = [\"PYTHON101\", \"DATA201\", \"ML301\", \"SQL101\", \"STATS201\", \"WEB101\"]\nbase_date = datetime(2024, 1, 1)\n\nprint(\"Generating dataset...\")\n\nfor i in range(1, 3001):  # 3000 student enrollments\n    student_id = random.randint(1000, 1500)  # 500 unique students\n    course_id = random.choice(courses)\n    \n    # Enrollment date (random day in 2024)\n    enroll_days_ago = random.randint(1, 365)\n    enrollment_date = (base_date + timedelta(days=enroll_days_ago)).strftime(\"%Y-%m-%d %H:%M:%S\")\n    \n    # Last activity (between enrollment and now)\n    activity_days_ago = random.randint(0, enroll_days_ago)\n    last_activity = (base_date + timedelta(days=enroll_days_ago) + \n                     timedelta(days=activity_days_ago)).strftime(\"%Y-%m-%d %H:%M:%S\")\n    \n    # Learning metrics\n    videos_watched = random.randint(0, 50)\n    quizzes_taken = random.randint(0, 15)\n    assignments_submitted = random.randint(0, 10)\n    forum_posts = random.randint(0, 30)\n    course_progress = round(random.uniform(0, 100), 2)\n    course_price = random.choice([49.99, 99.99, 149.99, 199.99])\n    \n    data.append((\n        student_id, course_id, enrollment_date, last_activity,\n        videos_watched, quizzes_taken, assignments_submitted,\n        forum_posts, course_progress, course_price\n    ))\n\n# Create DataFrame\ndf = spark.createDataFrame(data, schema=schema)\n\nprint(f\"Dataset created with {df.count()} records\")\nprint(f\"Number of unique students: {df.select('student_id').distinct().count()}\")\nprint(f\"Number of courses: {df.select('course_id').distinct().count()}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display first 10 rows\ndf.show(10, truncate=False)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display schema\ndf.printSchema()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 4: Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"# Course popularity\nprint(\"Course Popularity:\")\ncourse_popularity = (\n    df.groupBy('course_id')\n    .agg(count('student_id').alias('enrollments'))\n    .orderBy(desc('enrollments'))\n)\ncourse_popularity.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Engagement statistics\nprint(\"Average Engagement Metrics:\")\nengagement_stats = df.select(\n    round(avg('videos_watched'), 2).alias('avg_videos'),\n    round(avg('quizzes_taken'), 2).alias('avg_quizzes'),\n    round(avg('assignments_submitted'), 2).alias('avg_assignments'),\n    round(avg('course_progress'), 2).alias('avg_progress')\n)\nengagement_stats.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Revenue analysis\nprint(\"Total Revenue:\")\nrevenue = df.agg(\n    round(sum('course_price'), 2).alias('total_revenue')\n)\nrevenue.show()\n\nprint(\"\\nRevenue by Course:\")\nrevenue_by_course = (\n    df.groupBy('course_id')\n    .agg(\n        count('student_id').alias('enrollments'),\n        round(sum('course_price'), 2).alias('revenue')\n    )\n    .orderBy(desc('revenue'))\n)\nrevenue_by_course.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert dates to timestamps\ndf = df.withColumn(\n    \"enrollment_timestamp\",\n    to_timestamp(col(\"enrollment_date\"), \"yyyy-MM-dd HH:mm:ss\")\n)\n\ndf = df.withColumn(\n    \"activity_timestamp\",\n    to_timestamp(col(\"last_activity\"), \"yyyy-MM-dd HH:mm:ss\")\n)\n\n# Find date ranges\nprint(\"Date Ranges:\")\ndf.select(\n    min(\"enrollment_timestamp\").alias(\"first_enrollment\"),\n    max(\"enrollment_timestamp\").alias(\"last_enrollment\"),\n    max(\"activity_timestamp\").alias(\"most_recent_activity\")\n).show(truncate=False)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 5: Feature Engineering","metadata":{}},{"cell_type":"code","source":"# Calculate recency (days since last activity)\ndf = df.withColumn(\n    \"days_since_activity\",\n    datediff(current_timestamp(), col(\"activity_timestamp\"))\n)\n\n# For each student, get their most recent activity\nwindow_spec = Window.partitionBy(\"student_id\").orderBy(col(\"days_since_activity\"))\n\ndf_recency = (\n    df.withColumn(\"rank\", row_number().over(window_spec))\n    .filter(col(\"rank\") == 1)\n    .select(\"student_id\", col(\"days_since_activity\").alias(\"recency\"))\n)\n\nprint(\"Recency calculated:\")\ndf_recency.show(10)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate engagement score\ndf_engagement = df.groupBy('student_id').agg(\n    sum('videos_watched').alias('total_videos'),\n    sum('quizzes_taken').alias('total_quizzes'),\n    sum('assignments_submitted').alias('total_assignments'),\n    sum('forum_posts').alias('total_forum_posts'),\n    count('course_id').alias('courses_enrolled')\n)\n\n# Create composite engagement score\n# Weight: videos (1x), quizzes (2x), assignments (3x), forum (1.5x), courses (10x)\ndf_engagement = df_engagement.withColumn(\n    \"engagement_score\",\n    (col(\"total_videos\") * 1 +\n     col(\"total_quizzes\") * 2 +\n     col(\"total_assignments\") * 3 +\n     col(\"total_forum_posts\") * 1.5 +\n     col(\"courses_enrolled\") * 10)\n)\n\nprint(\"Engagement scores calculated:\")\ndf_engagement.show(10)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate performance metrics\ndf_performance = df.groupBy('student_id').agg(\n    round(avg('course_progress'), 2).alias('avg_progress'),\n    round(sum('course_price'), 2).alias('total_spent')\n)\n\nprint(\"Performance metrics calculated:\")\ndf_performance.show(10)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combine all features\ndf_features = (\n    df_recency\n    .join(df_engagement, on='student_id', how='inner')\n    .join(df_performance, on='student_id', how='inner')\n)\n\n# Select only the columns we need\ndf_final = df_features.select(\n    'student_id',\n    'recency',\n    'engagement_score',\n    'avg_progress',\n    'total_spent'\n).distinct()\n\nprint(f\"Final dataset has {df_final.count()} unique students\")\ndf_final.show(10)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 6: Data Standardization","metadata":{}},{"cell_type":"code","source":"# Assemble features into a vector\nassembler = VectorAssembler(\n    inputCols=[\"recency\", \"engagement_score\", \"avg_progress\", \"total_spent\"],\n    outputCol=\"features\"\n)\n\nassembled_data = assembler.transform(df_final)\n\nprint(\"Features assembled:\")\nassembled_data.select('student_id', 'features').show(5, truncate=False)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Standardize features\nscaler = StandardScaler(\n    inputCol=\"features\",\n    outputCol=\"scaled_features\",\n    withMean=True,\n    withStd=True\n)\n\nscaler_model = scaler.fit(assembled_data)\nscaled_data = scaler_model.transform(assembled_data)\n\nprint(\"Features standardized:\")\nscaled_data.select('student_id', 'scaled_features').show(5, truncate=False)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 7: K-Means Clustering - Finding Optimal k","metadata":{}},{"cell_type":"code","source":"# Test different numbers of clusters\ncosts = []\nK_range = range(2, 11)\n\nprint(\"Testing different values of k...\")\nfor k in K_range:\n    kmeans = KMeans(\n        featuresCol='scaled_features',\n        predictionCol='prediction',\n        k=k,\n        seed=42\n    )\n    \n    model = kmeans.fit(scaled_data)\n    cost = model.summary.trainingCost\n    costs.append(cost)\n    \n    print(f\"k={k}, Within-Cluster Sum of Squares={cost:.2f}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize the elbow curve\nplt.figure(figsize=(10, 6))\nplt.plot(K_range, costs, 'bo-', linewidth=2, markersize=8)\nplt.xlabel('Number of Clusters (k)', fontsize=12)\nplt.ylabel('Within-Cluster Sum of Squares', fontsize=12)\nplt.title('Elbow Method for Optimal k', fontsize=14, fontweight='bold')\nplt.grid(True, alpha=0.3)\nplt.xticks(K_range)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nLook for the 'elbow' in the curve - typically around k=3-5\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 8: Build Final K-Means Model","metadata":{}},{"cell_type":"code","source":"# Build final model with k=4 (adjust based on your elbow curve)\nkmeans = KMeans(\n    featuresCol='scaled_features',\n    predictionCol='cluster',\n    k=4,\n    seed=42\n)\n\nfinal_model = kmeans.fit(scaled_data)\npredictions = final_model.transform(scaled_data)\n\nprint(\"Model trained successfully!\")\nprint(\"\\nCluster assignments:\")\npredictions.select('student_id', 'recency', 'engagement_score', \n                   'avg_progress', 'total_spent', 'cluster').show(20)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 9: Cluster Analysis","metadata":{}},{"cell_type":"code","source":"# Calculate cluster statistics\ncluster_analysis = predictions.groupBy('cluster').agg(\n    count('student_id').alias('student_count'),\n    round(avg('recency'), 2).alias('avg_recency'),\n    round(avg('engagement_score'), 2).alias('avg_engagement'),\n    round(avg('avg_progress'), 2).alias('avg_progress'),\n    round(avg('total_spent'), 2).alias('avg_spent')\n).orderBy('cluster')\n\nprint(\"Cluster Statistics:\")\ncluster_analysis.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert to Pandas for visualization\ncluster_stats_pd = cluster_analysis.toPandas()\n\n# Create visualizations\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\nfig.suptitle('Student Cluster Characteristics', fontsize=16, fontweight='bold')\n\n# Plot 1: Recency\naxes[0, 0].bar(cluster_stats_pd['cluster'], cluster_stats_pd['avg_recency'], \n               color='skyblue', edgecolor='navy')\naxes[0, 0].set_title('Average Recency (Days Since Last Activity)', fontsize=11)\naxes[0, 0].set_xlabel('Cluster')\naxes[0, 0].set_ylabel('Days')\naxes[0, 0].grid(True, alpha=0.3)\n\n# Plot 2: Engagement Score\naxes[0, 1].bar(cluster_stats_pd['cluster'], cluster_stats_pd['avg_engagement'], \n               color='lightcoral', edgecolor='darkred')\naxes[0, 1].set_title('Average Engagement Score', fontsize=11)\naxes[0, 1].set_xlabel('Cluster')\naxes[0, 1].set_ylabel('Score')\naxes[0, 1].grid(True, alpha=0.3)\n\n# Plot 3: Progress\naxes[1, 0].bar(cluster_stats_pd['cluster'], cluster_stats_pd['avg_progress'], \n               color='lightgreen', edgecolor='darkgreen')\naxes[1, 0].set_title('Average Course Progress (%)', fontsize=11)\naxes[1, 0].set_xlabel('Cluster')\naxes[1, 0].set_ylabel('Progress %')\naxes[1, 0].grid(True, alpha=0.3)\n\n# Plot 4: Total Spent\naxes[1, 1].bar(cluster_stats_pd['cluster'], cluster_stats_pd['avg_spent'], \n               color='gold', edgecolor='orange')\naxes[1, 1].set_title('Average Total Spending ($)', fontsize=11)\naxes[1, 1].set_xlabel('Cluster')\naxes[1, 1].set_ylabel('Amount ($)')\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Interpret clusters\nprint(\"=\"*70)\nprint(\"CLUSTER INTERPRETATION\")\nprint(\"=\"*70)\n\nfor _, row in cluster_stats_pd.iterrows():\n    cluster_id = int(row['cluster'])\n    print(f\"\\n{'='*70}\")\n    print(f\"CLUSTER {cluster_id}: {int(row['student_count'])} students\")\n    print(f\"{'='*70}\")\n    print(f\"  Recency: {row['avg_recency']:.1f} days since last activity\")\n    print(f\"  Engagement: {row['avg_engagement']:.1f} points\")\n    print(f\"  Progress: {row['avg_progress']:.1f}%\")\n    print(f\"  Spending: ${row['avg_spent']:.2f}\")\n    \n    # Provide interpretation\n    if row['avg_recency'] < 30 and row['avg_engagement'] > 100:\n        profile = \"ðŸ“Š ACTIVE LEARNERS - Recent, highly engaged students\"\n        action = \"   ðŸ’¡ Action: Offer advanced courses and premium content\"\n    elif row['avg_progress'] > 70:\n        profile = \"ðŸ“Š HIGH PERFORMERS - Making excellent progress\"\n        action = \"   ðŸ’¡ Action: Recognize achievements, offer certifications\"\n    elif row['avg_recency'] > 60:\n        profile = \"ðŸ“Š AT-RISK STUDENTS - Haven't engaged recently\"\n        action = \"   ðŸ’¡ Action: Send re-engagement emails, offer discounts\"\n    else:\n        profile = \"ðŸ“Š CASUAL LEARNERS - Moderate engagement\"\n        action = \"   ðŸ’¡ Action: Encourage course completion, send reminders\"\n    \n    print(f\"  {profile}\")\n    print(f\"  {action}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 10: Actionable Insights","metadata":{}},{"cell_type":"code","source":"# Identify at-risk students\nat_risk_students = predictions.filter(\n    (col('recency') > 60) & (col('engagement_score') < 50)\n)\n\nprint(f\"At-risk students identified: {at_risk_students.count()}\")\nprint(\"\\nSample of at-risk students:\")\nat_risk_students.select('student_id', 'cluster', 'recency', \n                        'engagement_score', 'avg_progress').show(10)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Identify high-value students\nhigh_value_students = predictions.filter(\n    (col('total_spent') > 200) & (col('avg_progress') > 60)\n)\n\nprint(f\"High-value students: {high_value_students.count()}\")\nprint(\"\\nSample of high-value students:\")\nhigh_value_students.select('student_id', 'cluster', 'total_spent', \n                           'avg_progress', 'engagement_score').show(10)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Distribution of students across clusters\ncluster_distribution = predictions.groupBy('cluster').agg(\n    count('student_id').alias('count')\n).orderBy('cluster').toPandas()\n\nplt.figure(figsize=(10, 6))\nplt.pie(cluster_distribution['count'], \n        labels=[f'Cluster {i}' for i in cluster_distribution['cluster']],\n        autopct='%1.1f%%',\n        startangle=90,\n        colors=['skyblue', 'lightcoral', 'lightgreen', 'gold'])\nplt.title('Student Distribution Across Clusters', fontsize=14, fontweight='bold')\nplt.axis('equal')\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 11: Save Results","metadata":{}},{"cell_type":"code","source":"# Prepare final output\npredictions_summary = predictions.select(\n    'student_id',\n    'cluster',\n    'recency',\n    'engagement_score',\n    'avg_progress',\n    'total_spent'\n)\n\n# Save to CSV (uncomment to save)\n# predictions_summary.coalesce(1).write.mode('overwrite').csv(\n#     'student_segments.csv',\n#     header=True\n# )\n# print(\"Segmentation results saved to 'student_segments.csv'\")\n\n# Save cluster statistics\n# cluster_analysis.coalesce(1).write.mode('overwrite').csv(\n#     'cluster_statistics.csv',\n#     header=True\n# )\n# print(\"Cluster statistics saved to 'cluster_statistics.csv'\")\n\nprint(\"Analysis complete!\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 12: Stop Spark Session","metadata":{}},{"cell_type":"code","source":"# Clean up\n# spark.stop()\n# print(\"Spark session stopped.\")\n\nprint(\"Tutorial completed successfully!\")","metadata":{},"outputs":[],"execution_count":null}]}